---
title: "Project Capstone"
author: "Alberto Rossi"
date: "21/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Science Specialization - SwiftKey Capstone

# Overview
The goal of this capstone is to mimic the experience of being a data scientist, exploring an area called Natural Language Processing - NLP.
The idea is to apply the acquired knowledge and build a solution that, given a word initially typed, can return some terms and words that can be used / completed, in order to try to predict which word will be typed.


# Pre requirements
In order to be able to work with data and analysis, two extremely useful packages for NLP will be used:
- TM - For NLP
- RWeka - For Tokenization

```{r}
  library(tm)
  library(RWeka)
  library(tidyverse)
  set.seed(1906)
```


# Data loading
The reference database used to learn about words and their uses was provided by SwiftKey, a company specialized in the subject.
Database URL: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

```{r}

  # The zip file contains 4 folders, separared by language.
  # The instructions are to work only with English database. 
  dirsource <- "C:/RStudio/datasciencecoursera/Scripts/ProjectCapstone/final/en_US/"
  fileslist <- list.files(path=dirsource)
  #  l <- lapply(paste(dirsource, fileslist, sep="/"), function(f) {
  #  fsize <- file.info(f)[1]/1024/1024
  #  con <- file(f, open="r")
  #  lines <- readLines(con)
  #  nchars <- lapply(lines, nchar)
  #  maxchars <- which.max(nchars)
  #  nwords <- sum(sapply(strsplit(lines, "\\s+"), length))
  #  close(con)
  #  return(c(f, format(round(fsize, 2), nsmall=2), length(lines), maxchars, nwords))
  #})
  # save(filesummary, file="./Scripts/ProjectCapstone/filesummary.RData")
  #filesummary <- data.frame(matrix(unlist(l), nrow=length(l), byrow=T))
  load(file = "C:/RStudio/datasciencecoursera/Scripts/ProjectCapstone/filesummary.RData")
  colnames(filesummary) <- c("File", "Size(MB)", "Number lines", "Longest line", "Number Words")
  filesummary
```


# Pre-processing and Sampling
Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. 
Writing a function that takes a file as input and returns a tokenized version of it.

```{r}
  tokenForFile <- function(filepath) {
    conn <- file(filepath, "rb")
    document <- readLines(conn, encoding = "UTF-8")
    strsplit(x = document, split = "\\s+")
    close(conn)
  }
```

You might want to create a separate sub-sample dataset by reading in a random subset of the original data and writing it out to a separate file. That way, you can store the sample and not have to recreate it every time. You can use the rbinom function to "flip a biased coin" to determine whether you sample a line of text or not.

Lets sample 2% of each file, and group than in a single object.

```{r}  
  # First file
  file_data1 <- file(paste(dirsource, fileslist, sep="/")[1], open="r")
  file_lines1 <- readLines(file_data1)
  close(file_data1)
  
  # Second file
  file_data2 <- file(paste(dirsource, fileslist, sep="/")[2], open="r")
  file_lines2 <- readLines(file_data2)
  close(file_data2)
  
  # Third file
  file_data3 <- file(paste(dirsource, fileslist, sep="/")[3], open="r")
  file_lines3 <- readLines(file_data3)
  close(file_data3)

  file_sample1 = sample(file_lines1, length(file_lines1)/50)
  file_sample2 = sample(file_lines2, length(file_lines2)/50)
  file_sample3 = sample(file_lines3, length(file_lines3)/50)
  
  sample <- c(file_sample1, file_sample2, file_sample3)
  
  rm(file_lines1, file_sample1,
     file_lines2, file_sample2,
     file_lines3, file_sample3)
```


# Data cleaning

Basic remove symbols, punctuation, whitespace and blacklisted words (profanity) 

```{r}
  # Convert unwanted symbols in proper format
  samplecleaned = iconv(sample, "UTF-8","ASCii","byte")
  
  # Generate corpus of data
  samplecorpus = Corpus(VectorSource(samplecleaned))
  
  # Remove punctuation
  corpusdatacleaned = tm_map(samplecorpus,removePunctuation)
  
  # Convert data into lower case
  corpusdatacleaned = tm_map(corpusdatacleaned,content_transformer(tolower))
  
  # Remove numbers from the data
  corpusdatacleaned = tm_map(corpusdatacleaned,removeNumbers)
  
  # Remove whitespace
  sample = tm_map(corpusdatacleaned,stripWhitespace)
  rm(samplecorpus, corpusdatacleaned, samplecleaned)
  
  # Remove black listed word (Profanity cleaning)
  conn <- file("C:/RStudio/datasciencecoursera/Scripts/ProjectCapstone/blacklist.txt", "rb")
  profanity <- readLines(conn, encoding = "UTF-8")
  close(conn)
  sample <- tm_map(sample, removeWords, profanity) 
  rm(profanity)

```

# Data exploration

Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship betwee the words in the corpora.

Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

Tokenization process: breaking the cleaned sample into works to better work with than.
Lets break in 3 groups: one-part-words, two-part-words, three-part-words.


```{r}
  # Frequency - 1-part-word
  # Save part-word object in disk for cache -> better performance
  # grams1 <- NGramTokenizer(sample, Weka_control(min = 1, max = 1))
  # save(grams1, file="./Scripts/ProjectCapstone/grams1.RData")
  load(file = "C:/RStudio/datasciencecoursera/Scripts/ProjectCapstone/grams1.RData")
  
  words_dt <- data.frame(table(grams1))
  words_dt <- words_dt[order(words_dt$Freq,decreasing = TRUE),]
  words_dt <- words_dt[1:20,]
  names(words_dt) <- c("Grams","Freq")
  ggplot(words_dt, aes(x=reorder(Grams, Freq), y=Freq)) +
    geom_bar(stat = "identity") +  coord_flip() +
    xlab("1-part-words") + ylab("Frequency") +
    labs(title = "Most frequent one-gram-word")
  
  # Frequency - 2-part-word
  # Save part-word object in disk for cache -> better performance
  
  #grams2 <- NGramTokenizer(sample, Weka_control(min = 2, max = 2))
  #save(grams2, file="./Scripts/ProjectCapstone/grams2.RData")
  load(file = "C:/RStudio/datasciencecoursera/Scripts/ProjectCapstone/grams2.RData")
  
  words_dt <- data.frame(table(grams2))
  words_dt <- words_dt[order(words_dt$Freq,decreasing = TRUE),]
  words_dt <- words_dt[1:20,]
  names(words_dt) <- c("Grams","Freq")
  ggplot(words_dt, aes(x=reorder(Grams, Freq), y=Freq)) +
    geom_bar(stat = "identity") +  coord_flip() +
    xlab("2-part-words") + ylab("Frequency") +
    labs(title = "Most frequent two-gram-word")

  # Frequency - 3-part-word
  # Save part-word object in disk for cache -> better performance
  
  # grams3 <- NGramTokenizer(sample, Weka_control(min = 3, max = 3))
  # save(grams3, file="./Scripts/ProjectCapstone/grams3.RData")
  load(file = "C:/RStudio/datasciencecoursera/Scripts/ProjectCapstone/grams3.RData")
  
  words_dt <- data.frame(table(grams3))
  words_dt <- words_dt[order(words_dt$Freq,decreasing = TRUE),]
  words_dt <- words_dt[1:20,]
  names(words_dt) <- c("Grams","Freq")
  ggplot(words_dt, aes(x=reorder(Grams, Freq), y=Freq)) +
    geom_bar(stat = "identity") +  coord_flip() +
    xlab("3-part-words") + ylab("Frequency") +
    labs(title = "Most frequent three-gram-word")
```

How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?

```{r}
  # Data = NGram Tokenizer
  # Percentage Cover: words ratio to cover in data 
  wordCoverage <- function(data, percentageCover){
    words_dt <- data.frame(table(data))
    words_dt <- words_dt[order(words_dt$Freq,decreasing = TRUE),]
    
    pcover <- percentageCover*sum(words_dt$Freq)
    nwords <- 0
    for (i in 1:nrow(words_dt)) {
      if (nwords >= pcover) {
        return (i)
      }
      nwords <- nwords+words_dt$Freq[i]
    }
    
  }
  print("How many word to cover 50% of data - one-part-words:")
  wordCoverage(grams1, 0.5)
  print("How many word to cover 90% of data - two-word-words:")
  wordCoverage(grams1, 0.9)
```
